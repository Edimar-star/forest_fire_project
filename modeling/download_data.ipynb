{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUw1D4CkTx9v"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wJH_9Rekj_La"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from geopy.geocoders import Nominatim\n",
        "from sqlalchemy import create_engine\n",
        "from scipy.spatial import cKDTree\n",
        "from pyproj import Transformer\n",
        "from dotenv import load_dotenv\n",
        "from netCDF4 import Dataset\n",
        "from sqlalchemy import text\n",
        "from urllib import request\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import psycopg2\n",
        "import zipfile\n",
        "import math\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMT_BAVlIWT"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Fa4g6sOYlLM3"
      },
      "outputs": [],
      "source": [
        "def get_root_directory():\n",
        "    root_directory_path = \"../datasets\"\n",
        "\n",
        "    if not os.path.exists(root_directory_path):\n",
        "        os.makedirs(root_directory_path)\n",
        "\n",
        "    return root_directory_path\n",
        "\n",
        "def get_lat_lon_values(df_forest_fire):\n",
        "    lat_values, lon_values = df_forest_fire['latitude'].to_numpy(), df_forest_fire['longitude'].to_numpy()\n",
        "    lat_lon_values = list(set([(lat, lon) for lat, lon in zip(lat_values, lon_values)]))\n",
        "\n",
        "    return lat_lon_values\n",
        "\n",
        "def get_inter_extrapolated_values(df_forest_fire):\n",
        "    df_forest_fire['year'] = df_forest_fire['date'].astype(str).str.slice(start=0, stop=4).astype(int)\n",
        "    interpolated_values = df_forest_fire[df_forest_fire['year'] <= 2020][['latitude', 'longitude', 'year']].values\n",
        "    extrapolated_values = df_forest_fire[2021 <= df_forest_fire['year']][['latitude', 'longitude', 'year']].values\n",
        "    df_forest_fire = df_forest_fire.drop(columns=['year'])\n",
        "\n",
        "    return interpolated_values, extrapolated_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-GzYylUf8lp"
      },
      "source": [
        "# Forest Fire Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV3d7orqXusp"
      },
      "source": [
        "Near real-time (NRT) Moderate Resolution Imaging Spectroradiometer (MODIS) Thermal Anomalies / Fire locations - Collection 61 processed by NASA's Land, Atmosphere Near real-time Capability for EO (LANCE) Fire Information for Resource Management System (FIRMS), using swath products (MOD14/MYD14) rather than the tiled MOD14A1 and MYD14A1 products. The thermal anomalies / active fire represent the center of a 1 km pixel that is flagged by the MODIS MOD14/MYD14 Fire and Thermal Anomalies algorithm (Giglio 2003) as containing one or more fires within the pixel. This is the most basic fire product in which active fires and other thermal anomalies, such as volcanoes, are identified.\n",
        "\n",
        "For more information [here](https://www.earthdata.nasa.gov/learn/find-data/near-real-time/firms/mcd14dl-nrt#ed-firms-attributes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73bVkXzfo2mC"
      },
      "outputs": [],
      "source": [
        "def get_forest_fire_archive_dataset(root_directory_path, code):\n",
        "    remote_url = f\"https://firms.modaps.eosdis.nasa.gov/data/download/DL_FIRE_{code}.zip\"\n",
        "    local_file = f\"{root_directory_path}/forest_fire_Colombia.zip\"\n",
        "    remote_name = f\"{root_directory_path}/fire_archive_{code}.csv\"\n",
        "    request.urlretrieve(remote_url, local_file)\n",
        "\n",
        "    with zipfile.ZipFile(local_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(root_directory_path)\n",
        "        df_forest_fire = pd.read_csv(remote_name)\n",
        "\n",
        "    os.remove(f\"{root_directory_path}/fire_nrt_{code}.csv\")\n",
        "    os.remove(f\"{root_directory_path}/Readme.txt\")\n",
        "    os.remove(remote_name)\n",
        "    os.remove(local_file)\n",
        "\n",
        "    return df_forest_fire\n",
        "\n",
        "def download_forest_fire_dataset(root_directory_path):\n",
        "    df_viirs = get_forest_fire_archive_dataset(root_directory_path, \"SV-C2_457866\")\n",
        "    df_modis = get_forest_fire_archive_dataset(root_directory_path, \"M-C61_457865\")\n",
        "    df_forest_fire = pd.concat([df_viirs, df_modis])\n",
        "\n",
        "    return pd.merge(\n",
        "        df_forest_fire.sort_values(by=\"acq_date\")\n",
        "            .rename(columns={\"type\": \"fire_type\", \"acq_date\": \"date\"})\n",
        "            .dropna(),\n",
        "        pd.DataFrame({\n",
        "            \"fire_type\": [0, 1, 2, 3],\n",
        "            \"type\": [\"presumed vegetation fire\", \"active volcano\", \"other static land source\", \"offshore\"]\n",
        "        }), on=\"fire_type\", how=\"left\").drop(columns=[\"fire_type\"])\n",
        "\n",
        "def get_df_forest_fire(root_directory_path):\n",
        "    df_forest_fire = download_forest_fire_dataset(root_directory_path)\n",
        "    df_forest_fire.to_pickle(f\"{root_directory_path}/forest_fire.pkl\")\n",
        "    df_forest_fire['date'] = pd.to_datetime(df_forest_fire['date'])\n",
        "\n",
        "    return df_forest_fire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAV0R_YDgBen"
      },
      "source": [
        "# NDVI Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6HRYWtOZk1E"
      },
      "source": [
        "This dataset contains dekadal NDVI indicators computed from NASA's Moderate Resolution Imaging Spectroradiometer (MODIS) collection 6.1 from the Aqua and Terra satellite aggregated by sub-national administrative units.\n",
        "\n",
        "Included indicators are (for each dekad):\n",
        "\n",
        "- 10 day NDVI (vim)\n",
        "- NDVI long term average (vim_lta)\n",
        "- 10 day NDVI anomaly [%] (viq)\n",
        "\n",
        "The administrative units used for aggregation are based on WFP data and contain a Pcode reference attributed to each unit. The number of input pixels used to create the aggregates, is provided in the n_pixels column.\n",
        "\n",
        "More information [here](https://data.humdata.org/dataset/col-ndvi-subnational)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08KzB7OO5Nqu"
      },
      "outputs": [],
      "source": [
        "def download_ndvi_dataset(root_directory_path):\n",
        "    remote_url = \"https://data.humdata.org/dataset/7f2ba5ba-8df1-41cf-ab18-fc1da928a1e5/resource/c06298d9-0d4d-4e40-aecc-abc1da75dc4d/download/col-ndvi-adm2-full.csv\"\n",
        "    local_file_ndvi_dataset = f\"{root_directory_path}/ndvi_Colombia.csv\"\n",
        "    request.urlretrieve(remote_url, local_file_ndvi_dataset)\n",
        "\n",
        "    df_ndvi = pd.read_csv(local_file_ndvi_dataset, low_memory=False)\n",
        "    df_ndvi = df_ndvi.drop(df_ndvi.index[0])\n",
        "    df_ndvi['date'] = pd.to_datetime(df_ndvi['date'])\n",
        "    df_ndvi = df_ndvi[df_ndvi['date'] <= pd.to_datetime(\"2023-12-31\")]\n",
        "    os.remove(local_file_ndvi_dataset)\n",
        "\n",
        "    return df_ndvi\n",
        "\n",
        "def get_ndvi_postal_codes(df_ndvi, root_directory_path):\n",
        "    postal_codes = list(set(df_ndvi[\"ADM2_PCODE\"].values.astype('str')))\n",
        "    postal_codes = np.array([postal_code.replace(\"CO\", \"\") for postal_code in postal_codes]).astype(int)\n",
        "\n",
        "    # postal codes\n",
        "    remote_url = \"https://www.datos.gov.co/api/views/ixig-z8b5/rows.csv?accessType=DOWNLOAD\"\n",
        "    postal_codes_path = f\"{root_directory_path}/postal_codes.csv\"\n",
        "    request.urlretrieve(remote_url, postal_codes_path)\n",
        "\n",
        "    column_name = \"codigo_municipio\"\n",
        "    df_postal_codes = pd.read_csv(postal_codes_path)\n",
        "\n",
        "    df_postal_codes[column_name] = df_postal_codes[column_name].replace(',', '').astype(int)\n",
        "    df_postal_codes = df_postal_codes.drop_duplicates(subset=column_name, keep='first')\n",
        "\n",
        "    result = df_postal_codes[df_postal_codes[column_name].isin(postal_codes)][['nombre_departamento', 'nombre_municipio', 'codigo_municipio', 'codigo_postal']]\n",
        "    result.reset_index(drop=True, inplace=True)\n",
        "    result.sort_values(by=\"codigo_postal\")\n",
        "    os.remove(postal_codes_path)\n",
        "\n",
        "    return result\n",
        "\n",
        "def get_ndvi_lat_lon(geolocator, location):\n",
        "    try:\n",
        "        lat_lon_result = geolocator.geocode(location, timeout=10)\n",
        "        return (lat_lon_result.latitude, lat_lon_result.longitude) if lat_lon_result else (None, None)\n",
        "    except:\n",
        "        return (None, None)\n",
        "\n",
        "def get_ndvi_by_values(geolocator, locations, municipality_fixed):\n",
        "    lat_lon_ndvi = np.zeros((len(locations), 2))\n",
        "    for index, [municipality, department] in enumerate(locations):\n",
        "        municipality = municipality_fixed[municipality] if municipality in municipality_fixed else municipality\n",
        "        location = get_ndvi_lat_lon(geolocator, f\"{municipality}, {department}, COLOMBIA\")\n",
        "        lat_lon_ndvi[index] = np.array(location)\n",
        "\n",
        "    return lat_lon_ndvi\n",
        "\n",
        "def get_ndvi_lat_lon_values(result):\n",
        "    values = result[['nombre_municipio', 'nombre_departamento']].values\n",
        "    geolocator = Nominatim(user_agent=\"ndvi_data\")\n",
        "    municipality_fixed = {\n",
        "        \"VILLA DE SAN DIEGO DE UBATE\": \"UBATE\",\n",
        "        \"CERRO SAN ANTONIO\": \"SAN ANTONIO\",\n",
        "        \"SAN JUAN DE RIO SECO\": \"SAN JUAN DE RIOSECO\",\n",
        "        \"TOLU VIEJO\": \"TOLUVIEJO\",\n",
        "        \"SAN ANDRES DE TUMACO\": \"TUMACO\",\n",
        "        \"EL CANTON DEL SAN PABLO\": \"EL CANTON DE SAN PABLO\",\n",
        "        \"SAN LUIS DE SINCE\": \"SINCE\",\n",
        "        \"SAN JOSE DE ALBAN\": \"ALBAN\"\n",
        "    }\n",
        "\n",
        "    return get_ndvi_by_values(geolocator, values, municipality_fixed)\n",
        "\n",
        "def union_ndvi_data(df_ndvi, result, lat_lon_ndvi):\n",
        "    result['latitude'] = lat_lon_ndvi[:, 0]\n",
        "    result['longitude'] = lat_lon_ndvi[:, 1]\n",
        "\n",
        "    result.rename(columns={'codigo_municipio': 'ADM2_PCODE'}, inplace=True)\n",
        "    result['ADM2_PCODE'] = 'CO' + result['ADM2_PCODE'].astype(str)\n",
        "    merged_df_ndvi = pd.merge(df_ndvi, result[['latitude', 'longitude', 'ADM2_PCODE']], on='ADM2_PCODE', how='left')\n",
        "    merged_df_ndvi = merged_df_ndvi.drop(columns={'adm2_id', 'ADM2_PCODE'})\n",
        "\n",
        "    return merged_df_ndvi\n",
        "\n",
        "def collect_ndvi_data(root_directory_path, merged_df_ndvi, df_forest_fire):\n",
        "    values = {key: [] for key in merged_df_ndvi.columns}\n",
        "    merged_df_ndvi = merged_df_ndvi.dropna()\n",
        "\n",
        "    for year in range(2002, 2024):\n",
        "        # Filtramos los datos\n",
        "        date_min, date_max = pd.to_datetime(f'{year}-01-01'), pd.to_datetime(f'{year}-12-31')\n",
        "        df_ndvi_temp = merged_df_ndvi[(date_min <= merged_df_ndvi['date']) & (merged_df_ndvi['date'] <= date_max)]\n",
        "        df_forest_fire_temp = df_forest_fire[(date_min <= df_forest_fire['date']) & (df_forest_fire['date'] <= date_max)]\n",
        "\n",
        "        df_ndvi_temp.reset_index(drop=True, inplace=True)\n",
        "        df_forest_fire_temp.reset_index(drop=True, inplace=True)\n",
        "        init_date = pd.to_datetime(f'{year}-01-01')\n",
        "\n",
        "        # forest fire values\n",
        "        lat_values = df_forest_fire_temp['latitude'].values\n",
        "        lon_values = df_forest_fire_temp['longitude'].values\n",
        "        time_values = (df_forest_fire_temp['date'] - init_date).dt.days.values\n",
        "\n",
        "        # ndvi values\n",
        "        lat = df_ndvi_temp['latitude'].values\n",
        "        lon = df_ndvi_temp['longitude'].values\n",
        "        time = (df_ndvi_temp['date'] - init_date).dt.days.values\n",
        "\n",
        "        points = np.vstack((lat, lon, time)).T\n",
        "        tree = cKDTree(points)\n",
        "        query_points = np.vstack((lat_values, lon_values, time_values)).T\n",
        "        _, indexes = tree.query(query_points)\n",
        "\n",
        "        for key in ['latitude', 'longitude', 'date']:\n",
        "            values[key] += list(df_forest_fire_temp[key].values)\n",
        "\n",
        "        for key in ['n_pixels', 'vim', 'vim_avg', 'viq']:\n",
        "            values[key] = np.append(values[key], df_ndvi_temp.iloc[indexes][key].values).astype(float)\n",
        "\n",
        "    df_ndvi = pd.DataFrame(values).sort_values(by=\"date\").dropna()\n",
        "    df_ndvi.to_pickle(f\"{root_directory_path}/ndvi.pkl\")\n",
        "\n",
        "def save_df_ndvi(root_directory_path, df_forest_fire):\n",
        "    df_ndvi = download_ndvi_dataset(root_directory_path)\n",
        "    result = get_ndvi_postal_codes(df_ndvi, root_directory_path)\n",
        "\n",
        "    lat_lon_ndvi = get_ndvi_lat_lon_values(result)\n",
        "    merged_df_ndvi = union_ndvi_data(df_ndvi, result, lat_lon_ndvi)\n",
        "\n",
        "    collect_ndvi_data(root_directory_path, merged_df_ndvi, df_forest_fire)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OrmyxAWgPpT"
      },
      "source": [
        "# Global Climate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOuBpgxjfxVp"
      },
      "source": [
        "TerraClimate is a dataset of monthly climate and climatic water balance for global terrestrial surfaces from 1958-2019. These data provide important inputs for ecological and hydrological studies at global scales that require high spatial resolution and time-varying data. All data have monthly temporal resolution and a ~4-km (1/24th degree) spatial resolution. The data cover the period from 1958-2020. We plan to update these data periodically (annually).\n",
        "\n",
        "More information [here](https://www.climatologylab.org/terraclimate.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQzPGTwmV2dp"
      },
      "outputs": [],
      "source": [
        "def check_latlon_bounds(lat,lon,lat_index,lon_index,lat_target,lon_target):\n",
        "    #check final indices are in right bounds\n",
        "    if(lat[lat_index]>lat_target):\n",
        "        if(lat_index!=0):\n",
        "            lat_index = lat_index - 1\n",
        "    if(lat[lat_index]<lat_target):\n",
        "        if(lat_index!=len(lat)):\n",
        "            lat_index = lat_index +1\n",
        "    if(lon[lon_index]>lon_target):\n",
        "        if(lon_index!=0):\n",
        "            lon_index = lon_index - 1\n",
        "    if(lon[lon_index]<lon_target):\n",
        "        if(lon_index!=len(lon)):\n",
        "            lon_index = lon_index + 1\n",
        "\n",
        "    return [lat_index, lon_index]\n",
        "\n",
        "def get_indexes(data, points):\n",
        "    data_reshaped = data.filled().reshape(-1, 1)\n",
        "    tree = cKDTree(data_reshaped)\n",
        "    query_points = points.to_numpy().reshape(-1, 1)\n",
        "    _, indexes = tree.query(query_points)\n",
        "\n",
        "    return indexes\n",
        "\n",
        "def get_data_by_date(varname, filehandle, time_values, lat_values, lon_values, year, lat_min, lon_min, lat_max, lon_max):\n",
        "    # subset in space (lat/lon)\n",
        "    lathandle = filehandle.variables['lat']\n",
        "    lonhandle = filehandle.variables['lon']\n",
        "    lat=lathandle[:]\n",
        "    lon=lonhandle[:]\n",
        "\n",
        "    # find indices of target lat/lon/day\n",
        "    lat_index_min = (np.abs(lat-lat_min)).argmin()\n",
        "    lat_index_max = (np.abs(lat-lat_max)).argmin()\n",
        "    lon_index_min = (np.abs(lon-lon_min)).argmin()\n",
        "    lon_index_max = (np.abs(lon-lon_max)).argmin()\n",
        "\n",
        "    [lat_index_min,lon_index_min] = check_latlon_bounds(lat, lon, lat_index_min, lon_index_min, lat_min, lon_min)\n",
        "    [lat_index_max,lon_index_max] = check_latlon_bounds(lat, lon, lat_index_max, lon_index_max, lon_max, lon_max)\n",
        "\n",
        "    if(lat_index_min>lat_index_max):\n",
        "        lat_index_range = range(lat_index_max, lat_index_min+1)\n",
        "    else:\n",
        "        lat_index_range = range(lat_index_min, lat_index_max+1)\n",
        "    if(lon_index_min>lon_index_max):\n",
        "        lon_index_range = range(lon_index_max, lon_index_min+1)\n",
        "    else:\n",
        "        lon_index_range = range(lon_index_min, lon_index_max+1)\n",
        "\n",
        "    lat=lat[lat_index_range]\n",
        "    lon=lon[lon_index_range]\n",
        "\n",
        "    # subset in time\n",
        "    timehandle=filehandle.variables['time']\n",
        "    time=timehandle[:]\n",
        "    time_min = (date(year,1,1)-date(1900,1,1)).days\n",
        "    time_max = (date(year,12,31)-date(1900,1,1)).days\n",
        "    time_index_min = (np.abs(time-time_min)).argmin()\n",
        "    time_index_max = (np.abs(time-time_max)).argmin()\n",
        "    time_index_range = range(time_index_min, time_index_max+1)\n",
        "    time = timehandle[time_index_range]\n",
        "\n",
        "    # subset data\n",
        "    datahandle = filehandle.variables[varname]\n",
        "    data = datahandle[time_index_range, lat_index_range, lon_index_range]\n",
        "\n",
        "    # Indexes\n",
        "    time_indexes = get_indexes(time, time_values)\n",
        "    lat_indexes = get_indexes(lat, lat_values)\n",
        "    lon_indexes = get_indexes(lon, lon_values)\n",
        "\n",
        "    return list(data[time_indexes, lat_indexes, lon_indexes].filled(np.nan))\n",
        "\n",
        "def get_data_country(df_modis, varnames, datasets):\n",
        "    values = {varname: [] for varname in [\"date\", \"latitude\", \"longitude\"] + varnames}\n",
        "    df_modis[\"date\"] = pd.to_datetime(df_modis[\"date\"])\n",
        "    df_modis = df_modis.sort_values(by=\"date\")\n",
        "\n",
        "    for year in range(2002, 2024):\n",
        "        df = df_modis[df_modis[\"date\"] <= pd.to_datetime(f\"{year}-12-31\")]\n",
        "        df = df[pd.to_datetime(f\"{year}-01-01\") <= df[\"date\"]]\n",
        "\n",
        "        date_values, lat_values, lon_values = df['date'], df['latitude'], df['longitude']\n",
        "        lat_min, lon_min = lat_values.min(), lon_values.min()\n",
        "        lat_max, lon_max = lat_values.max(), lon_values.max()\n",
        "\n",
        "        values['date'] += [str(date_.date()) for date_ in date_values]\n",
        "        values['latitude'] += list(lat_values.values)\n",
        "        values['longitude'] += list(lon_values.values)\n",
        "        time_values = (date_values - pd.to_datetime(\"1900-01-01\")).dt.days\n",
        "\n",
        "        for varname in varnames:\n",
        "            filehandle = datasets[f\"{year}-{varname}\"]\n",
        "            values[varname] += get_data_by_date(varname, filehandle, time_values, lat_values, lon_values, year, lat_min, lon_min, lat_max, lon_max)\n",
        "\n",
        "    return values\n",
        "\n",
        "def download_global_climate_dataset(varnames):\n",
        "    datasets = {}\n",
        "    for year in range(2002, 2024):\n",
        "        for varname in varnames:\n",
        "            pathname = f\"http://thredds.northwestknowledge.net:8080/thredds/dodsC/TERRACLIMATE_ALL/data/TerraClimate_{varname}_{year}.nc\"\n",
        "            filehandle = Dataset(pathname, 'r', format=\"NETCDF4\")\n",
        "            datasets[f\"{year}-{varname}\"] = filehandle\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def save_df_global_climate(root_directory_path, df_forest_fire):\n",
        "    varnames = [\"ws\", \"vpd\", \"vap\", \"tmin\", \"tmax\", \"swe\", \"srad\", \"soil\", \"q\", \"ppt\", \"pet\", \"def\", \"aet\", \"PDSI\"]\n",
        "    datasets = download_global_climate_dataset(varnames)\n",
        "    values = get_data_country(df_forest_fire, varnames, datasets)\n",
        "    df_global_climate = pd.DataFrame(values)\n",
        "    for varname in varnames:\n",
        "        df_global_climate[varname] = df_global_climate[varname].astype(float, copy=True)\n",
        "\n",
        "    # Convertimos las temperaturas a kelvin\n",
        "    kelvin = 273.15\n",
        "    df_global_climate[\"tmin\"] = df_global_climate[\"tmin\"] + kelvin\n",
        "    df_global_climate[\"tmax\"] = df_global_climate[\"tmax\"] + kelvin\n",
        "\n",
        "    # Guardamos el dataset\n",
        "    df_global_climate['date'] = pd.to_datetime(df_global_climate['date'])\n",
        "    df_global_climate = df_global_climate.sort_values(by=\"date\").dropna()\n",
        "    df_global_climate.to_pickle(f\"{root_directory_path}/global_climate.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQo1TWpPTvn_"
      },
      "source": [
        "# Land cover data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ8xknDv_hFE"
      },
      "source": [
        "The Intergovernmental Panel on Climate Change (IPCC) provides guidance on reporting areal extent and change of land cover and land use, requiring the use of estimators that neither over or underestimate dynamics to the degree possible, and that have known uncertainties. The maps provided by GLAD do not have these properties. However, the maps can be leveraged to facilitate appropriate probability-based statistical methods in deriving statistically valid areas of forest extent and change. Specifically, the maps may be used as a stratifier in targeting forest extent and/or change by a probability sample. The team at GLAD has demonstrated such approaches using the GLAD forest loss data in sample-based area estimation (Tyukavina et al., ERL, 2018, Turubanova et al., ERL, 2019, and Potapov et al., RSE, 2019, among others).\n",
        "\n",
        "More information [here](https://storage.googleapis.com/earthenginepartners-hansen/GLCLU2000-2020/v2/download.html)\n",
        "\n",
        "Legend [here](https://storage.googleapis.com/earthenginepartners-hansen/GLCLU2000-2020/legend.xlsx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA3f5FmDlU9I"
      },
      "outputs": [],
      "source": [
        "def download_land_cover_dataset(root_directory_path):\n",
        "    range_values = [(20, 80), (10, 80), (10, 70), ('00', 80), ('00', 70)]\n",
        "    for year in range(2000, 2021, 5):\n",
        "        for N, W in range_values:\n",
        "            remote_url = f\"https://storage.googleapis.com/earthenginepartners-hansen/GLCLU2000-2020/v2/{year}/{N}N_0{W}W.tif\"\n",
        "            land_cover_path = f\"{root_directory_path}/land_cover_Colombia_{year}_{N}N_0{W}W.tif\"\n",
        "            request.urlretrieve(remote_url, land_cover_path)\n",
        "\n",
        "    return range_values\n",
        "\n",
        "def get_land_cover(lat_lon_array, land_cover_path):\n",
        "    with rasterio.open(land_cover_path) as src:\n",
        "        transform = src.transform\n",
        "        tif_crs = src.crs\n",
        "        transformer = Transformer.from_crs(\"epsg:4326\", tif_crs, always_xy=True)\n",
        "        lon_values, lat_values = lat_lon_array[:,1], lat_lon_array[:,0]\n",
        "        x_coords, y_coords = transformer.transform(lon_values, lat_values)\n",
        "        row, col = rasterio.transform.rowcol(transform, x_coords, y_coords)\n",
        "        values = src.read(1)[row, col]\n",
        "\n",
        "    return lat_values, lon_values, values\n",
        "\n",
        "def save_land_cover_values(values, name, root_directory_path):\n",
        "    land_cover_values = {'lat': [], 'lon': [], 'year': [], 'land_cover': []}\n",
        "    lat_lon_array = np.array(values)\n",
        "    for year in range(2000, 2021, 5):\n",
        "        land_cover_path = f\"{root_directory_path}/land_cover_Colombia_{year}_{name}.tif\"\n",
        "        lat_values, lon_values, result = get_land_cover(lat_lon_array, land_cover_path)\n",
        "\n",
        "        land_cover_values['lat'] += list(lat_values)\n",
        "        land_cover_values['lon'] += list(lon_values)\n",
        "        land_cover_values['year'] += list(np.full(len(lat_values), year))\n",
        "        land_cover_values['land_cover'] += list(result)\n",
        "\n",
        "        os.remove(f\"{root_directory_path}/land_cover_Colombia_{year}_{name}.tif\")\n",
        "\n",
        "    df = pd.DataFrame(land_cover_values)\n",
        "    df.to_csv(f\"{root_directory_path}/land_cover_Colombia_{name}.csv\", index=False)\n",
        "\n",
        "def split_lat_lon_values(lat_lon_values, range_values, root_directory_path):\n",
        "    limits = [\n",
        "        ((10, 20), (-80, -70)),\n",
        "        ((0, 10), (-80, -70)),\n",
        "        ((0, 10), (-70, -60)),\n",
        "        ((-10, 0), (-80, -70)),\n",
        "        ((-10, 0), (-70, -60))\n",
        "    ]\n",
        "    for limit_values, (N, W) in zip(limits, range_values):\n",
        "        ((lat_min, lat_max), (lon_min, lon_max)), name = limit_values, f\"{N}N_0{W}W\"\n",
        "        lat_lon_values_filtered = list(filter(lambda lat_lon: lat_min < lat_lon[0] <= lat_max and lon_min <= lat_lon[1] < lon_max, lat_lon_values))\n",
        "        save_land_cover_values(lat_lon_values_filtered, name, root_directory_path)\n",
        "\n",
        "def union_land_cover_data(range_values, root_directory_path):\n",
        "    df_land_cover = pd.DataFrame()\n",
        "    for N, W in range_values:\n",
        "        land_cover_csv_path = f\"{root_directory_path}/land_cover_Colombia_{N}N_0{W}W.csv\"\n",
        "        df_tlc = pd.read_csv(land_cover_csv_path)\n",
        "        df_land_cover = pd.concat([df_land_cover, df_tlc])\n",
        "        os.remove(land_cover_csv_path)\n",
        "\n",
        "    return df_land_cover.sort_values(by=\"year\").dropna()\n",
        "\n",
        "def get_model(df_land_cover):\n",
        "    X = df_land_cover[['lat', 'lon', 'year']].values\n",
        "    y = df_land_cover['land_cover'].values\n",
        "\n",
        "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = DecisionTreeRegressor(max_depth=30, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "def save_df_land_cover_predicted(root_directory_path, model, interpolated_values, extrapolated_values):\n",
        "    land_covers_interpolated = model.predict(interpolated_values)\n",
        "    land_covers_extrapolated = model.predict(extrapolated_values)\n",
        "\n",
        "    df_land_cover_predicted = pd.DataFrame({\n",
        "        'latitude': np.append(interpolated_values[:, 0], extrapolated_values[:, 0]),\n",
        "        'longitude': np.append(interpolated_values[:, 1], extrapolated_values[:, 1]),\n",
        "        'year': np.append(interpolated_values[:, 2], extrapolated_values[:, 2]).astype(int),\n",
        "        'land_cover': np.append(land_covers_interpolated, land_covers_extrapolated).astype(int)\n",
        "    }).sort_values(by=\"year\")\n",
        "    df_land_cover_predicted.to_pickle(f\"{root_directory_path}/land_cover.pkl\")\n",
        "\n",
        "def download_land_cover_legend(root_directory_path):\n",
        "    remote_url = f\"https://storage.googleapis.com/earthenginepartners-hansen/GLCLU2000-2020/legend.xlsx\"\n",
        "    land_cover_legend_path = f\"{root_directory_path}/land_cover_legend_Colombia.xlsx\"\n",
        "    request.urlretrieve(remote_url, land_cover_legend_path)\n",
        "\n",
        "    return land_cover_legend_path\n",
        "\n",
        "def set_values(df_land_cover_legend, column1, column2, indexes, nan_indexes=[]):\n",
        "    for start_index, end_index in indexes:\n",
        "        total = end_index - start_index + 1\n",
        "        df_land_cover_legend.loc[np.linspace(start_index, end_index, total), column1] = df_land_cover_legend.at[start_index, column2]\n",
        "\n",
        "    for nan_index in nan_indexes:\n",
        "        df_land_cover_legend.at[nan_index, column1] = np.NAN\n",
        "\n",
        "def save_df_land_cover_legend(root_directory_path, land_cover_legend_path):\n",
        "    df_land_cover_legend = pd.read_excel(land_cover_legend_path)\n",
        "    df_land_cover_legend = df_land_cover_legend.drop(columns={\"Color code\"}).rename(columns={'Unnamed: 2': 'class'})\n",
        "\n",
        "    # Same column\n",
        "    set_values(df_land_cover_legend, 'General class', 'General class', [(0, 96), (100, 196), (200, 207)], [97, 197, 208, 242, 245, 251, 255])\n",
        "    set_values(df_land_cover_legend, 'class', 'class', [(0, 1), (2, 18), (19, 24), (25, 48), (100, 101), (102, 118), (119, 124), (125, 148)], [49, 149])\n",
        "\n",
        "    # Other column\n",
        "    set_values(df_land_cover_legend, 'class', 'General class', [(200, 207), (241, 241), (244, 244), (250, 250), (254, 254)])\n",
        "    set_values(df_land_cover_legend, 'Sub-class', 'General class', [(241, 241), (244, 244), (250, 250), (254, 254)])\n",
        "\n",
        "    # Replacing nan values\n",
        "    df_land_cover_legend = df_land_cover_legend.fillna(\"Not registered\")\n",
        "    df_land_cover_legend.to_pickle(f\"{root_directory_path}/land_cover_legend.pkl\")\n",
        "    os.remove(land_cover_legend_path)\n",
        "\n",
        "def save_df_land_cover(root_directory_path, df_forest_fire):\n",
        "    interpolated_values, extrapolated_values = get_inter_extrapolated_values(df_forest_fire)\n",
        "    lat_lon_values = get_lat_lon_values(df_forest_fire)\n",
        "\n",
        "    range_values = download_land_cover_dataset(root_directory_path)\n",
        "    split_lat_lon_values(lat_lon_values, range_values, root_directory_path)\n",
        "\n",
        "    df_land_cover = union_land_cover_data(range_values, root_directory_path)\n",
        "    land_cover_legend_path = download_land_cover_legend(root_directory_path)\n",
        "    model = get_model(df_land_cover)\n",
        "\n",
        "    save_df_land_cover_predicted(root_directory_path, model, interpolated_values, extrapolated_values)\n",
        "    save_df_land_cover_legend(root_directory_path, land_cover_legend_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E---mq6HTxAs"
      },
      "source": [
        "# Population Density Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHjz39TQaj70"
      },
      "source": [
        "Estimated population density per grid-cell. The dataset is available to download in Geotiff and ASCII XYZ format at a resolution of 30 arc (approximately 1km at the equator). The projection is Geographic Coordinate System, WGS84. The units are number of people per square kilometre based on country totals adjusted to match the corresponding official United Nations population estimates that have been prepared by the Population Division of the Department of Economic and Social Affairs of the United Nations Secretariat (2019 Revision of World Population Prospects). The mapping approach is Random Forest-based dasymetric redistribution.\n",
        "\n",
        "More information [here](https://hub.worldpop.org/geodata/summary?id=45716)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4TWwCcwWmza"
      },
      "outputs": [],
      "source": [
        "def download_population_density_dataset(root_directory_path):\n",
        "    for year in range(2002, 2021):\n",
        "        remote_url = f\"https://data.worldpop.org/GIS/Population_Density/Global_2000_2020_1km/{year}/COL/col_pd_{year}_1km_ASCII_XYZ.zip\"\n",
        "        local_file = f\"{root_directory_path}/population_density_Colombia_{year}.zip\"\n",
        "        request.urlretrieve(remote_url, local_file)\n",
        "\n",
        "        with zipfile.ZipFile(local_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(root_directory_path)\n",
        "        os.remove(local_file)\n",
        "\n",
        "def save_data(root_directory_path, df_forest_fire):\n",
        "    for year in range(2002, 2021):\n",
        "        pd_path = f\"{root_directory_path}/col_pd_{year}_1km_ASCII_XYZ.csv\"\n",
        "        if os.path.exists(pd_path):\n",
        "            df_pd = pd.read_csv(pd_path)\n",
        "            df_pd.rename(columns={'X': 'longitude', 'Y': 'latitude', 'Z': 'population_density'}, inplace=True)\n",
        "\n",
        "            # Filtramos por fecha\n",
        "            date_min, date_max = pd.to_datetime(f\"{year}\"), pd.to_datetime(f\"{year + 1}\")\n",
        "            df_ff = df_forest_fire[(date_min <= df_forest_fire['date']) & (df_forest_fire['date'] < date_max)]\n",
        "            lat_values, lon_values = df_ff['latitude'], df_ff['longitude']\n",
        "\n",
        "            # Minimos\n",
        "            lat_min, lat_max = lat_values.min(), lat_values.max()\n",
        "            lon_min, lon_max = lon_values.min(), lon_values.max()\n",
        "\n",
        "            # Filtramos las latitudes\n",
        "            df_pd.sort_values(by=\"latitude\")\n",
        "            df_pd = df_pd[lat_min <= df_pd['latitude']]\n",
        "            df_pd = df_pd[df_pd['latitude'] <= lat_max]\n",
        "\n",
        "            # Filtramos las longitudes\n",
        "            df_pd.sort_values(by=\"longitude\")\n",
        "            df_pd = df_pd[lon_min <= df_pd['longitude']]\n",
        "            df_pd = df_pd[df_pd['longitude'] <= lon_max]\n",
        "\n",
        "            # Establecemos valores\n",
        "            df_pd.reset_index(drop=True, inplace=True)\n",
        "            lat, lon = df_pd['latitude'].to_numpy(), df_pd['longitude'].to_numpy()\n",
        "\n",
        "            # Hallamos los valores\n",
        "            points = np.vstack((lat, lon)).T\n",
        "            tree = cKDTree(points)\n",
        "            query_points = np.vstack((lat_values, lon_values)).T\n",
        "            _, indices = tree.query(query_points)\n",
        "            population_density_values = df_pd.iloc[indices]['population_density'].to_numpy()\n",
        "\n",
        "            # Guardamos los datos\n",
        "            df_population_density = pd.DataFrame({'latitude': lat_values, 'longitude': lon_values,\n",
        "                                        'year': np.full(len(lat_values), year), 'population_density': population_density_values})\n",
        "            df_population_density.to_csv(f\"{root_directory_path}/population_density_Colombia_{year}.csv\", index=False)\n",
        "            os.remove(pd_path)\n",
        "\n",
        "def union_pd_data(root_directory_path):\n",
        "    df_population_density = pd.DataFrame()\n",
        "    for year in range(2002, 2021):\n",
        "        df_ppd = pd.read_csv(f\"{root_directory_path}/population_density_Colombia_{year}.csv\")\n",
        "        df_population_density = pd.concat([df_population_density, df_ppd])\n",
        "        os.remove(f\"{root_directory_path}/population_density_Colombia_{year}.csv\")\n",
        "\n",
        "    return df_population_density.sort_values(by=\"year\").dropna()\n",
        "\n",
        "def get_regressor(df_population_density):\n",
        "    X = df_population_density[['latitude', 'longitude', 'year']].values  # Características: año, latitud y longitud\n",
        "    y = df_population_density['population_density'].values             # Densidad de población como variable dependiente\n",
        "\n",
        "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    regressor = DecisionTreeRegressor(max_depth=20, random_state=42)\n",
        "    regressor.fit(X_train, y_train)\n",
        "\n",
        "    return regressor\n",
        "\n",
        "def save_df_population_density_predicted(root_directory_path, df_population_density, regressor, extrapolated_values):\n",
        "    densities_predicted = regressor.predict(extrapolated_values)\n",
        "    df_pd_predicted = pd.DataFrame({\n",
        "        'latitude': extrapolated_values[:, 0],\n",
        "        'longitude': extrapolated_values[:, 1],\n",
        "        'year': extrapolated_values[:, 2].astype(int),\n",
        "        'population_density': densities_predicted\n",
        "    })\n",
        "\n",
        "    df_pd_predicted = pd.concat([df_population_density, df_pd_predicted]).sort_values(by=\"year\")\n",
        "    df_pd_predicted.to_pickle(f\"{root_directory_path}/population_density.pkl\")\n",
        "\n",
        "def save_df_population_density(root_directory_path, df_forest_fire):\n",
        "    download_population_density_dataset(root_directory_path)\n",
        "    save_data(root_directory_path, df_forest_fire)\n",
        "\n",
        "    _, extrapolated_values = get_inter_extrapolated_values(df_forest_fire)\n",
        "    df_population_density = union_pd_data(root_directory_path)\n",
        "    regressor = get_regressor(df_population_density)\n",
        "\n",
        "    save_df_population_density_predicted(root_directory_path, df_population_density, regressor, extrapolated_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKC5J75yliZq"
      },
      "source": [
        "# Dem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnPKVBitlrOD"
      },
      "source": [
        "he Land Processes Distributed Active Archive Center (LP DAAC) is responsible for the archive and distribution of the NASA Making Earth System Data Records for Use in Research Environments (MEaSUREs) version SRTM, which includes the global 1 arc second (~30 meter) product. NASA Shuttle Radar Topography Mission (SRTM) datasets result from a collaborative effort by the National Aeronautics and Space Administration (NASA) and the National Geospatial-Intelligence Agency (NGA - previously known as the National Imagery and Mapping Agency, or NIMA), as well as the participation of the German and Italian space agencies. The purpose of SRTM was to generate a near-global digital elevation model (DEM) of the Earth using radar interferometry. SRTM was a primary component of the payload on the Space Shuttle Endeavour during its STS-99 mission.\n",
        "\n",
        "More information [here](https://cmr.earthdata.nasa.gov/search/concepts/C1000000240-LPDAAC_ECS.html)\n",
        "\n",
        "Download dataset [here](https://opendap.cr.usgs.gov/opendap/hyrax/DP109/SRTM/SRTMGL1.003/2000.02.11/contents.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoMlt7fQlmx5"
      },
      "outputs": [],
      "source": [
        "def download_hgt_file(polygon, root_directory_path):\n",
        "    try:\n",
        "        file_name = f\"{root_directory_path}/file_{polygon}.zip\"\n",
        "        url = f\"https://opendap.cr.usgs.gov/opendap/hyrax/DP109/SRTM/SRTMGL1.003/2000.02.11/{polygon}.SRTMGL1.hgt.zip\"\n",
        "        request.urlretrieve(url, file_name)\n",
        "\n",
        "        with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "            zip_ref.extractall(root_directory_path)\n",
        "\n",
        "        os.remove(file_name)\n",
        "\n",
        "        return f\"{root_directory_path}/{polygon}.hgt\"\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def read_hgt_file(file_path):\n",
        "    nrows, ncols = (3601, 3601)\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        elevations = np.fromfile(f, dtype='>i2', count=nrows*ncols).reshape((nrows, ncols))\n",
        "        elevations = elevations.astype(np.float32)\n",
        "        elevations[elevations == -32768] = np.nan\n",
        "\n",
        "    os.remove(file_path)\n",
        "    return elevations\n",
        "\n",
        "def get_lat_lon_bounds(latitudes, longitudes):\n",
        "    lat_min, lon_min = latitudes.min(), longitudes.min()\n",
        "    lat_max, lon_max = latitudes.max(), longitudes.max()\n",
        "\n",
        "    lat_min, lon_min = math.floor(lat_min), math.floor(lon_min)\n",
        "    lat_max, lon_max = math.ceil(lat_max), math.floor(lon_max)\n",
        "\n",
        "    return (lat_min, lon_min), (lat_max, lon_max)\n",
        "\n",
        "def get_geo_values(lat_min, lon_min, total, arc_length=3600):\n",
        "    lat_values = lat_min + np.linspace(0, total) / arc_length\n",
        "    lon_values = lon_min + np.linspace(0, total) / arc_length\n",
        "\n",
        "    return lat_values, lon_values\n",
        "\n",
        "def get_changes(polygon, root_directory_path, lat_min, lon_min, latitudes, longitudes, min_distances, values):\n",
        "    hgt_file_name = download_hgt_file(polygon, root_directory_path)\n",
        "\n",
        "    if hgt_file_name is None:\n",
        "        return min_distances, values\n",
        "\n",
        "    elevation_values = read_hgt_file(hgt_file_name)\n",
        "    elevations = np.array(elevation_values).flatten()\n",
        "\n",
        "    lat, lon = get_geo_values(lat_min, lon_min, len(elevations))\n",
        "    distances, indexes = get_distances_and_indexes(lat, lon, latitudes, longitudes)\n",
        "\n",
        "    if len(values) == 0:\n",
        "        return np.array(distances), elevations[indexes]\n",
        "    else:\n",
        "        result_indexes = np.where(distances < min_distances)[0]\n",
        "        if len(result_indexes) > 0:\n",
        "            values[result_indexes] = elevations[indexes][result_indexes]\n",
        "            min_distances[result_indexes] = distances[result_indexes]\n",
        "\n",
        "        return min_distances, values\n",
        "\n",
        "def download_dem_data(root_directory_path, latitudes, longitudes, min_bounds, max_bounds):\n",
        "    lat_min, lon_min = min_bounds\n",
        "    lat_max, lon_max = max_bounds\n",
        "\n",
        "    values = np.array([])\n",
        "    min_distances = np.full(len(latitudes), np.inf)\n",
        "    for i in range(lat_max - lat_min + 1):\n",
        "        for j in range(lon_max - lon_min + 1):\n",
        "            r = \"S0\" if lat_min + i < 0 else \"N0\" if lat_min + i < 10 else \"N\"\n",
        "            polygon = f\"{r}{abs(lat_min + i)}W0{abs(lon_min + j)}\"\n",
        "            min_distances, values = get_changes(polygon, root_directory_path, lat_min + i, lon_min + j, latitudes, longitudes, min_distances.copy(), values.copy())\n",
        "\n",
        "    return values\n",
        "\n",
        "def get_distances_and_indexes(lat, lon, lat_values, lon_values):\n",
        "    points = np.vstack((lat, lon)).T\n",
        "    tree = cKDTree(points)\n",
        "    query_points = np.vstack((lat_values, lon_values)).T\n",
        "    distances, indexes = tree.query(query_points)\n",
        "    tree = None\n",
        "\n",
        "    return distances, indexes\n",
        "\n",
        "def get_minimum_differences(root_directory_path, df_forest_fire):\n",
        "    lat_lon_values = np.array(get_lat_lon_values(df_forest_fire))\n",
        "    latitudes, longitudes = lat_lon_values[:, 0], lat_lon_values[:, 1]\n",
        "\n",
        "    min_bounds, max_bounds = get_lat_lon_bounds(latitudes, longitudes)\n",
        "    values = download_dem_data(root_directory_path, latitudes, longitudes, min_bounds, max_bounds)\n",
        "\n",
        "    return latitudes, longitudes, values\n",
        "\n",
        "def save_df_dem(root_directory_path, df_forest_fire):\n",
        "    latitudes, longitudes, values = get_minimum_differences(root_directory_path, df_forest_fire)\n",
        "    df_dem = pd.DataFrame({'latitude': latitudes, 'longitude': longitudes, 'dem': values})\n",
        "    df_dem.to_pickle(f\"{root_directory_path}/dem.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntTltNEbm26J"
      },
      "source": [
        "# Union Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR4xuDhdm4iT"
      },
      "outputs": [],
      "source": [
        "def read_values(root_directory_path):\n",
        "    df_dem = pd.read_pickle(f\"{root_directory_path}/dem.pkl\")\n",
        "    df_ndvi = pd.read_pickle(f\"{root_directory_path}/ndvi.pkl\")\n",
        "\n",
        "    df_land_cover = pd.read_pickle(f\"{root_directory_path}/land_cover.pkl\")\n",
        "    df_forest_fire = pd.read_pickle(f\"{root_directory_path}/forest_fire.pkl\")\n",
        "    df_global_climate = pd.read_pickle(f\"{root_directory_path}/global_climate.pkl\")\n",
        "\n",
        "    df_land_cover_legend = pd.read_pickle(f\"{root_directory_path}/land_cover_legend.pkl\")\n",
        "    df_population_density = pd.read_pickle(f\"{root_directory_path}/population_density.pkl\")\n",
        "\n",
        "    return df_dem, df_ndvi, df_land_cover, df_forest_fire, df_global_climate, df_land_cover_legend, df_population_density\n",
        "\n",
        "def union_data(root_directory_path):\n",
        "    df_dem, df_ndvi, df_land_cover, df_forest_fire, df_global_climate, df_land_cover_legend, df_population_density = read_values(root_directory_path)\n",
        "\n",
        "    df_forest_fire['date'] = pd.to_datetime(df_forest_fire['date'])\n",
        "    df_forest_fire['year'] = df_forest_fire['date'].astype(str).str.slice(start=0, stop=4).astype(int)\n",
        "\n",
        "    df_final = pd.merge(df_forest_fire, df_dem, on=['latitude', 'longitude'], how=\"left\")\n",
        "    df_final = pd.merge(df_final, df_ndvi, on=['latitude', 'longitude', 'date'], how=\"left\")\n",
        "\n",
        "    df_final_land_cover = pd.merge(df_land_cover.rename(columns={'land_cover': 'Map value'}), df_land_cover_legend, on=['Map value'], how=\"left\").drop(columns=['Map value'])\n",
        "    df_final = pd.merge(df_final, df_final_land_cover, on=['latitude', 'longitude', 'year'], how=\"left\")\n",
        "\n",
        "    df_final = pd.merge(df_final, df_global_climate, on=['latitude', 'longitude', 'date'], how=\"left\")\n",
        "    df_final = pd.merge(df_final, df_population_density, on=['latitude', 'longitude', 'year'], how=\"left\")\n",
        "\n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaWpSGWVmVbj"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sNyjhd6mWtn"
      },
      "outputs": [],
      "source": [
        "def download_data(root_directory_path):\n",
        "    df_forest_fire = get_df_forest_fire(root_directory_path)\n",
        "    save_df_ndvi(root_directory_path, df_forest_fire.copy())\n",
        "\n",
        "    save_df_global_climate(root_directory_path, df_forest_fire.copy())\n",
        "    save_df_land_cover(root_directory_path, df_forest_fire.copy())\n",
        "\n",
        "    save_df_population_density(root_directory_path, df_forest_fire.copy())\n",
        "    save_df_dem(root_directory_path, df_forest_fire.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fng1WU2Potpa"
      },
      "source": [
        "# Save data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpeDF7rXmpKR"
      },
      "outputs": [],
      "source": [
        "root_directory_path = get_root_directory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDGILH61mrHF"
      },
      "outputs": [],
      "source": [
        "download_data(root_directory_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "NyqvH2i0mvkg",
        "outputId": "1ff12f9c-65ea-4dae-c1a5-9db02e3635cf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_final"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ff5c6b71-1a2d-4a3f-a476-b7c3f0de115d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>brightness</th>\n",
              "      <th>scan</th>\n",
              "      <th>track</th>\n",
              "      <th>date</th>\n",
              "      <th>acq_time</th>\n",
              "      <th>satellite</th>\n",
              "      <th>instrument</th>\n",
              "      <th>confidence</th>\n",
              "      <th>...</th>\n",
              "      <th>swe</th>\n",
              "      <th>srad</th>\n",
              "      <th>soil</th>\n",
              "      <th>q</th>\n",
              "      <th>ppt</th>\n",
              "      <th>pet</th>\n",
              "      <th>def</th>\n",
              "      <th>aet</th>\n",
              "      <th>PDSI</th>\n",
              "      <th>population_density</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.9371</td>\n",
              "      <td>-73.5798</td>\n",
              "      <td>309.9</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1.1</td>\n",
              "      <td>2002-07-01</td>\n",
              "      <td>1542</td>\n",
              "      <td>Terra</td>\n",
              "      <td>MODIS</td>\n",
              "      <td>48</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>217.5</td>\n",
              "      <td>241.5</td>\n",
              "      <td>4.8</td>\n",
              "      <td>95.4</td>\n",
              "      <td>136.8</td>\n",
              "      <td>17.0</td>\n",
              "      <td>119.8</td>\n",
              "      <td>2.07</td>\n",
              "      <td>14.648424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.7785</td>\n",
              "      <td>-75.8046</td>\n",
              "      <td>306.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2002-07-01</td>\n",
              "      <td>1543</td>\n",
              "      <td>Terra</td>\n",
              "      <td>MODIS</td>\n",
              "      <td>61</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>171.7</td>\n",
              "      <td>39.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>94.7</td>\n",
              "      <td>98.5</td>\n",
              "      <td>3.2</td>\n",
              "      <td>95.3</td>\n",
              "      <td>1.28</td>\n",
              "      <td>443.014130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.2584</td>\n",
              "      <td>-74.0905</td>\n",
              "      <td>333.5</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.1</td>\n",
              "      <td>2002-07-01</td>\n",
              "      <td>1542</td>\n",
              "      <td>Terra</td>\n",
              "      <td>MODIS</td>\n",
              "      <td>65</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>231.3</td>\n",
              "      <td>140.4</td>\n",
              "      <td>5.3</td>\n",
              "      <td>105.3</td>\n",
              "      <td>154.4</td>\n",
              "      <td>31.3</td>\n",
              "      <td>123.1</td>\n",
              "      <td>1.52</td>\n",
              "      <td>13.135686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.4153</td>\n",
              "      <td>-74.3193</td>\n",
              "      <td>321.7</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.1</td>\n",
              "      <td>2002-07-01</td>\n",
              "      <td>1542</td>\n",
              "      <td>Terra</td>\n",
              "      <td>MODIS</td>\n",
              "      <td>68</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>233.6</td>\n",
              "      <td>128.8</td>\n",
              "      <td>5.9</td>\n",
              "      <td>117.2</td>\n",
              "      <td>154.9</td>\n",
              "      <td>28.8</td>\n",
              "      <td>126.1</td>\n",
              "      <td>1.25</td>\n",
              "      <td>9.273766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9.6186</td>\n",
              "      <td>-74.5987</td>\n",
              "      <td>321.3</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2002-07-01</td>\n",
              "      <td>1542</td>\n",
              "      <td>Terra</td>\n",
              "      <td>MODIS</td>\n",
              "      <td>69</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>218.9</td>\n",
              "      <td>215.2</td>\n",
              "      <td>7.2</td>\n",
              "      <td>144.4</td>\n",
              "      <td>139.6</td>\n",
              "      <td>1.1</td>\n",
              "      <td>138.5</td>\n",
              "      <td>1.54</td>\n",
              "      <td>15.801455</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 39 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff5c6b71-1a2d-4a3f-a476-b7c3f0de115d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ff5c6b71-1a2d-4a3f-a476-b7c3f0de115d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ff5c6b71-1a2d-4a3f-a476-b7c3f0de115d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-afdc9e3b-a806-4977-962a-dda907c6313b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-afdc9e3b-a806-4977-962a-dda907c6313b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-afdc9e3b-a806-4977-962a-dda907c6313b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   latitude  longitude  brightness  scan  track       date  acq_time  \\\n",
              "0    8.9371   -73.5798       309.9   1.3    1.1 2002-07-01      1542   \n",
              "1    4.7785   -75.8046       306.0   1.1    1.0 2002-07-01      1543   \n",
              "2   10.2584   -74.0905       333.5   1.1    1.1 2002-07-01      1542   \n",
              "3   10.4153   -74.3193       321.7   1.1    1.1 2002-07-01      1542   \n",
              "4    9.6186   -74.5987       321.3   1.1    1.0 2002-07-01      1542   \n",
              "\n",
              "  satellite instrument confidence  ...  swe   srad   soil    q    ppt    pet  \\\n",
              "0     Terra      MODIS         48  ...  0.0  217.5  241.5  4.8   95.4  136.8   \n",
              "1     Terra      MODIS         61  ...  0.0  171.7   39.0  4.7   94.7   98.5   \n",
              "2     Terra      MODIS         65  ...  0.0  231.3  140.4  5.3  105.3  154.4   \n",
              "3     Terra      MODIS         68  ...  0.0  233.6  128.8  5.9  117.2  154.9   \n",
              "4     Terra      MODIS         69  ...  0.0  218.9  215.2  7.2  144.4  139.6   \n",
              "\n",
              "    def    aet  PDSI  population_density  \n",
              "0  17.0  119.8  2.07           14.648424  \n",
              "1   3.2   95.3  1.28          443.014130  \n",
              "2  31.3  123.1  1.52           13.135686  \n",
              "3  28.8  126.1  1.25            9.273766  \n",
              "4   1.1  138.5  1.54           15.801455  \n",
              "\n",
              "[5 rows x 39 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_final = union_data(root_directory_path)\n",
        "df_final.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H9R8mWLnFvx"
      },
      "outputs": [],
      "source": [
        "df_final.to_pickle(f\"{root_directory_path}/final_dataset.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOxMF-N-nKzT"
      },
      "source": [
        "# Data to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t0EtgpznL0u",
        "outputId": "0ce70bb7-c565-4a9c-cb16-e344243eb4d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK-YkKz2nQdH"
      },
      "outputs": [],
      "source": [
        "drive_path = \"/content/drive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd4fll9KnSf9"
      },
      "outputs": [],
      "source": [
        "root_directory_drive_path = f\"{drive_path}/datasets\"\n",
        "\n",
        "if not os.path.exists(root_directory_drive_path):\n",
        "    os.makedirs(root_directory_drive_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SboA7wnEnUxb"
      },
      "outputs": [],
      "source": [
        "def save_df_to_drive(root_directory_path, root_directory_drive_path, dataset_name):\n",
        "    df = pd.read_pickle(f\"{root_directory_path}/{dataset_name}.pkl\")\n",
        "    df.to_pickle(f\"{root_directory_drive_path}/{dataset_name}.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7EH1TLUnXVx"
      },
      "outputs": [],
      "source": [
        "dataset_names = [\n",
        "    \"dem\", \"ndvi\", \"land_cover\", \"forest_fire\", \"global_climate\",\n",
        "    \"land_cover_legend\", \"population_density\", \"final_dataset\"\n",
        "]\n",
        "\n",
        "for dataset_name in dataset_names:\n",
        "    save_df_to_drive(root_directory_path, root_directory_drive_path, dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jEDw3U1RwOX"
      },
      "source": [
        "# Data to database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RChnalEeSuU-",
        "outputId": "afff5bd9-ff79-421f-e4ee-642e9e3985b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "qlKFEhP7Rxf9"
      },
      "outputs": [],
      "source": [
        "POSTGRES_USER = os.getenv('POSTGRES_USER')\n",
        "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
        "POSTGRES_HOST = os.getenv('POSTGRES_HOST')\n",
        "POSTGRES_PORT = os.getenv('POSTGRES_PORT')\n",
        "POSTGRES_DB = os.getenv('POSTGRES_DB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "siwGpVwhR2_x"
      },
      "outputs": [],
      "source": [
        "engine = create_engine(f'postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB4zheQMR4Fq"
      },
      "outputs": [],
      "source": [
        "Session = sessionmaker(bind=engine)\n",
        "session = Session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0e0yzm5SNV3"
      },
      "outputs": [],
      "source": [
        "TABLE_NAME = os.getenv('TABLE_NAME')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6pn3DjPSGn7"
      },
      "outputs": [],
      "source": [
        "df_final.to_sql(TABLE_NAME, engine, if_exists='replace', index=False, method='multi', chunksize=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfhC85pDSFH1"
      },
      "outputs": [],
      "source": [
        "sql_query = text(f\"ALTER TABLE {TABLE_NAME} ALTER COLUMN date TYPE DATE USING date::date;\")\n",
        "session.execute(sql_query)\n",
        "session.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HGKLA3LR56G"
      },
      "outputs": [],
      "source": [
        "session.close()\n",
        "engine.dispose()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lUw1D4CkTx9v",
        "bSMT_BAVlIWT",
        "Q-GzYylUf8lp",
        "WAV0R_YDgBen",
        "5OrmyxAWgPpT",
        "JQo1TWpPTvn_",
        "E---mq6HTxAs",
        "cKC5J75yliZq",
        "ntTltNEbm26J",
        "XaWpSGWVmVbj",
        "fng1WU2Potpa",
        "UOxMF-N-nKzT",
        "9jEDw3U1RwOX"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
